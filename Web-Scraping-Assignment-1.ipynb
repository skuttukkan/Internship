{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74390da9",
   "metadata": {},
   "source": [
    "#### 1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3619c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_wikipedia_headers(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content)\n",
    "    \n",
    "    headers = []\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"mw-headline\"):\n",
    "        headers.append(i.text)\n",
    "    \n",
    "    df = pd.DataFrame(headers, columns=['Headers'])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1f6342e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Headers\n",
      "0           Welcome to Wikipedia\n",
      "1  From today's featured article\n",
      "2               Did you know ...\n",
      "3                    In the news\n",
      "4                    On this day\n",
      "5       Today's featured picture\n",
      "6       Other areas of Wikipedia\n",
      "7    Wikipedia's sister projects\n",
      "8            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "df = scrape_wikipedia_headers(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb45d77",
   "metadata": {},
   "source": [
    "#### 2) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "fb43a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Presidents  \\\n",
      "0           Shri Ram Nath Kovind (birth - 1945)   \n",
      "1             Shri Pranab Mukherjee (1935-2020)   \n",
      "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
      "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
      "4            Shri K. R. Narayanan (1920 - 2005)   \n",
      "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
      "6               Shri R Venkataraman (1910-2009)   \n",
      "7                  Giani Zail Singh (1916-1994)   \n",
      "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
      "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
      "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
      "11                 Dr. Zakir Husain (1897-1969)   \n",
      "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
      "13             Dr. Rajendra Prasad (1884-1963)    \n",
      "\n",
      "                                       Term of Office  \n",
      "0   25 July, 2017 to 25 July, 2022 \\nhttps://ramna...  \n",
      "1   25 July, 2012 to 25 July, 2017 \\nhttp://pranab...  \n",
      "2   25 July, 2007 to 25 July, 2012 \\nhttp://pratib...  \n",
      "3   25 July, 2002 to 25 July, 2007 \\nhttp://abdulk...  \n",
      "4                      25 July, 1997 to 25 July, 2002  \n",
      "5                      25 July, 1992 to 25 July, 1997  \n",
      "6                      25 July, 1987 to 25 July, 1992  \n",
      "7                      25 July, 1982 to 25 July, 1987  \n",
      "8                      25 July, 1977 to 25 July, 1982  \n",
      "9                24 August, 1974 to 11 February, 1977  \n",
      "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
      "11                        13 May, 1967 to 3 May, 1969  \n",
      "12                       13 May, 1962 to 13 May, 1967  \n",
      "13                   26 January, 1950 to 13 May, 1962  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def president_names(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    names = []\n",
    "    terms = []\n",
    "    \n",
    "    for i in soup.find_all('div', class_=\"presidentListing\"):\n",
    "        text = i.text.strip()\n",
    "        parts = text.split('\\nTerm of Office: ')\n",
    "        if len(parts) == 2:\n",
    "            name, term = parts\n",
    "            term = term.split(' (')[0]  # Remove the link part\n",
    "        else:\n",
    "            name, term = parts[0], None\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "\n",
    "    df = pd.DataFrame({'Presidents': names, 'Term of Office': terms})\n",
    "    return df\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "df = president_names(url)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f43db0e",
   "metadata": {},
   "source": [
    "#### 3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "#### a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1f3afb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank         Teams Matches Points Rating\n",
      "0    1     Australia      23  2,714    118\n",
      "1    2      Pakistan      20  2,316    116\n",
      "2    3         India      33  3,807    115\n",
      "3    4   New Zealand      27  2,806    104\n",
      "4    5       England      24  2,426    101\n",
      "5    6  South Africa      19  1,910    101\n",
      "6    7    Bangladesh      25  2,451     98\n",
      "7    8   Afghanistan      10    878     88\n",
      "8    9     Sri Lanka      21  1,682     80\n",
      "9   10   West Indies      25  1,797     72\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def top_mteam(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    ranking=[]\n",
    "    # Get the first ranking from a different class\n",
    "    first_ranking_element = soup.find('td', class_='rankings-block__banner--pos')\n",
    "    first_ranking = first_ranking_element.text.strip()\n",
    "    ranking.append(first_ranking)\n",
    "    \n",
    "    # Get the remaining rankings\n",
    "    ranking_elements = soup.find_all('td', class_='table-body__cell table-body__cell--position u-text-right')\n",
    "    for r in ranking_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ranking.append(r.text.strip())\n",
    "     \n",
    "    teams = []\n",
    "    for i, team in enumerate(soup.find_all('span', class_='u-hide-phablet'), 1):\n",
    "        if i > 10:\n",
    "            break\n",
    "        teams.append(team.text.strip())\n",
    "    \n",
    "    matches=[]\n",
    "    # Get the first matches from a different class\n",
    "    first_matches_element = soup.find('td', class_='rankings-block__banner--matches')\n",
    "    first_matches = first_matches_element.text.strip()\n",
    "    matches.append(first_matches)\n",
    "        \n",
    "    points=[]\n",
    "    # Get the first points from a different class\n",
    "    first_points_element = soup.find('td', class_='rankings-block__banner--points')\n",
    "    first_points = first_points_element.text.strip()\n",
    "    points.append(first_points)\n",
    "    \n",
    "    table_rows = soup.find_all('tr')\n",
    "\n",
    "    for row in table_rows[1:]:\n",
    "        columns = row.find_all('td', class_='table-body__cell u-center-text')\n",
    "\n",
    "        if len(columns) >= 2:\n",
    "            matches.append(columns[0].text.strip())\n",
    "            points.append(columns[1].text.strip())\n",
    "            if len(matches) == 10 and len(points) == 10:\n",
    "                break\n",
    "                \n",
    "    ratings = []\n",
    "    \n",
    "    # Get the first rating from a different class\n",
    "    first_rating_element = soup.find('td', class_='rankings-block__banner--rating u-text-right')\n",
    "    first_rating = first_rating_element.text.strip()\n",
    "    ratings.append(first_rating)\n",
    "    \n",
    "    # Get the remaining ratings\n",
    "    rank_elements = soup.find_all('td', class_=\"table-body__cell u-text-right rating\")\n",
    "    for rank in rank_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ratings.append(rank.text.strip())\n",
    "    \n",
    "    df = pd.DataFrame({'Rank':ranking,'Teams':teams,'Matches': matches,'Points':points,'Rating':ratings})\n",
    "    return df\n",
    "\n",
    "url=\"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "df = top_mteam(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c292bb",
   "metadata": {},
   "source": [
    "#### 3) b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "d9b83fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank                 Player Team Rating\n",
      "0    1             Babar Azam  PAK    886\n",
      "1    2  Rassie van der Dussen   SA    777\n",
      "2    3           Fakhar Zaman  PAK    755\n",
      "3    4            Imam-ul-Haq  PAK    745\n",
      "4    5           Shubman Gill  IND    738\n",
      "5    6           David Warner  AUS    726\n",
      "6    7           Harry Tector  IRE    722\n",
      "7    8            Virat Kohli  IND    719\n",
      "8    9        Quinton de Kock   SA    718\n",
      "9   10           Rohit Sharma  IND    707\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def top_batsmen(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "    ranking=[]\n",
    "    # Get the first ranking from a different class\n",
    "    first_ranking_element = soup.find('span', class_='rankings-block__pos-number')\n",
    "    first_ranking = first_ranking_element.text.strip()\n",
    "    ranking.append(first_ranking)\n",
    "    \n",
    "    # Get the remaining rankings\n",
    "    ranking_elements = soup.find_all('span', class_=\"rankings-table__pos-number\")\n",
    "    for r in ranking_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ranking.append(r.text.strip())\n",
    "    \n",
    "    player=[]\n",
    "     # Get the first player from a different class\n",
    "    first_player_element = soup.find('div', class_=\"rankings-block__banner--name-large\")\n",
    "    first_player = first_player_element.text.strip()\n",
    "    player.append(first_player)\n",
    "    \n",
    "    # Get the remaining players\n",
    "    player_elements = soup.find_all('td', class_=\"table-body__cell rankings-table__name name\")\n",
    "    for p in player_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        player.append(p.text.strip())\n",
    "    \n",
    "    teams=[]\n",
    "    # Get the first player team from a different class\n",
    "    first_team_element = soup.find('div', class_=\"rankings-block__banner--nationality\")\n",
    "    first_team = first_team_element.text.strip()\n",
    "    teams.append(first_team)\n",
    "    \n",
    "    # Get the remaining teams\n",
    "    team_elements = soup.find_all('span', class_='table-body__logo-text')\n",
    "    for t in team_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        teams.append(t.text.strip())\n",
    "    \n",
    "   \n",
    "    ratings = []\n",
    "    \n",
    "    # Get the first rating from a different class\n",
    "    first_rating_element = soup.find('div', class_='rankings-block__banner--rating')\n",
    "    first_rating = first_rating_element.text.strip()\n",
    "    ratings.append(first_rating)\n",
    "    \n",
    "    # Get the remaining ratings\n",
    "    rank_elements = soup.find_all('td', class_=\"table-body__cell rating\")\n",
    "    for rank in rank_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ratings.append(rank.text.strip())\n",
    "    \n",
    "    df = pd.DataFrame({'Rank': ranking,'Player':player,\"Team\":teams,'Rating':ratings})\n",
    "    return df\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "df = top_batsmen(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957d953",
   "metadata": {},
   "source": [
    "#### 3) c) Top 10 ODI bowlers along with the records of their team andrating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "de064eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank            Player Team Rating\n",
      "0    1    Josh Hazlewood  AUS    705\n",
      "1    2    Mohammed Siraj  IND    691\n",
      "2    3    Mitchell Starc  AUS    686\n",
      "3    4        Matt Henry   NZ    667\n",
      "4    5       Trent Boult   NZ    660\n",
      "5    6       Rashid Khan  AFG    659\n",
      "6    7        Adam Zampa  AUS    652\n",
      "7    8  Mujeeb Ur Rahman  AFG    637\n",
      "8    9     Mohammad Nabi  AFG    631\n",
      "9   10    Shaheen Afridi  PAK    630\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def top_bowler(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "    ranking=[]\n",
    "    # Get the first ranking from a different class\n",
    "    first_ranking_element = soup.find('span', class_='rankings-block__pos-number')\n",
    "    first_ranking = first_ranking_element.text.strip()\n",
    "    ranking.append(first_ranking)\n",
    "    \n",
    "    # Get the remaining rankings\n",
    "    ranking_elements = soup.find_all('span', class_=\"rankings-table__pos-number\")\n",
    "    for r in ranking_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ranking.append(r.text.strip())\n",
    "    \n",
    "    player=[]\n",
    "     # Get the first player from a different class\n",
    "    first_player_element = soup.find('div', class_=\"rankings-block__banner--name-large\")\n",
    "    first_player = first_player_element.text.strip()\n",
    "    player.append(first_player)\n",
    "    \n",
    "    # Get the remaining players\n",
    "    player_elements = soup.find_all('td', class_=\"table-body__cell rankings-table__name name\")\n",
    "    for p in player_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        player.append(p.text.strip())\n",
    "    \n",
    "    teams=[]\n",
    "    # Get the first player team from a different class\n",
    "    first_team_element = soup.find('div', class_=\"rankings-block__banner--nationality\")\n",
    "    first_team = first_team_element.text.strip()\n",
    "    teams.append(first_team)\n",
    "    \n",
    "    # Get the remaining teams\n",
    "    team_elements = soup.find_all('span', class_='table-body__logo-text')\n",
    "    for t in team_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        teams.append(t.text.strip())\n",
    "    \n",
    "   \n",
    "    ratings = []\n",
    "    \n",
    "    # Get the first rating from a different class\n",
    "    first_rating_element = soup.find('div', class_='rankings-block__banner--rating')\n",
    "    first_rating = first_rating_element.text.strip()\n",
    "    ratings.append(first_rating)\n",
    "    \n",
    "    # Get the remaining ratings\n",
    "    rank_elements = soup.find_all('td', class_=\"table-body__cell rating\")\n",
    "    for rank in rank_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ratings.append(rank.text.strip())\n",
    "    \n",
    "    df = pd.DataFrame({'Rank': ranking,'Player':player,\"Team\":teams,'Rating':ratings})\n",
    "    return df\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "df = top_bowler(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687ec6a9",
   "metadata": {},
   "source": [
    "#### 4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\n",
    "\n",
    "#### a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "13de1ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank         Teams Matches Points Rating\n",
      "0    1     Australia      21  3,603    172\n",
      "1    2       England      28  3,342    119\n",
      "2    3  South Africa      26  3,098    119\n",
      "3    4         India      27  2,820    104\n",
      "4    5   New Zealand      25  2,553    102\n",
      "5    6   West Indies      27  2,535     94\n",
      "6    7      Thailand      11    821     75\n",
      "7    8    Bangladesh      14    977     70\n",
      "8    9      Pakistan      27  1,678     62\n",
      "9   10     Sri Lanka       9    479     53\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def top_wteam(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    ranking=[]\n",
    "    # Get the first ranking from a different class\n",
    "    first_ranking_element = soup.find('td', class_='rankings-block__banner--pos')\n",
    "    first_ranking = first_ranking_element.text.strip()\n",
    "    ranking.append(first_ranking)\n",
    "    \n",
    "    # Get the remaining rankings\n",
    "    ranking_elements = soup.find_all('td', class_=\"table-body__cell table-body__cell--position u-text-right\")\n",
    "    for r in ranking_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ranking.append(r.text.strip())\n",
    "    \n",
    "    teams = []\n",
    "    for i, team in enumerate(soup.find_all('span', class_=\"u-hide-phablet\"), 1):\n",
    "        if i > 10:\n",
    "            break\n",
    "        teams.append(team.text.strip())\n",
    "    ratings = []\n",
    "    \n",
    "    # Get the first rating from a different class\n",
    "    first_rating_element = soup.find('td', class_='rankings-block__banner--rating u-text-right')\n",
    "    first_rating = first_rating_element.text.strip()\n",
    "    ratings.append(first_rating)\n",
    "    \n",
    "    # Get the remaining ratings\n",
    "    rank_elements = soup.find_all('td', class_=\"table-body__cell u-text-right rating\")\n",
    "    for rank in rank_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ratings.append(rank.text.strip())\n",
    "    \n",
    "    matches=[]\n",
    "    # Get the first matches from a different class\n",
    "    first_matches_element = soup.find('td', class_='rankings-block__banner--matches')\n",
    "    first_matches = first_matches_element.text.strip()\n",
    "    matches.append(first_matches)\n",
    "        \n",
    "    points=[]\n",
    "    # Get the first points from a different class\n",
    "    first_points_element = soup.find('td', class_='rankings-block__banner--points')\n",
    "    first_points = first_points_element.text.strip()\n",
    "    points.append(first_points)\n",
    "    \n",
    "    table_rows = soup.find_all('tr')\n",
    "\n",
    "    for row in table_rows[1:]:\n",
    "        columns = row.find_all('td', class_='table-body__cell u-center-text')\n",
    "\n",
    "        if len(columns) >= 2:\n",
    "            matches.append(columns[0].text.strip())\n",
    "            points.append(columns[1].text.strip())\n",
    "            if len(matches) == 10 and len(points) == 10:\n",
    "                break\n",
    "   \n",
    "    df = pd.DataFrame({'Rank':ranking,'Teams': teams,'Matches': matches,'Points':points,'Rating':ratings})\n",
    "    return df\n",
    "\n",
    "url=\"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "df = top_wteam(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89630d9",
   "metadata": {},
   "source": [
    "#### 4) b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "015406bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank               Player Team Rating\n",
      "0    1          Beth Mooney  AUS    754\n",
      "1    2      Laura Wolvaardt   SA    732\n",
      "2    3       Natalie Sciver  ENG    731\n",
      "3    4          Meg Lanning  AUS    717\n",
      "4    5     Harmanpreet Kaur  IND    716\n",
      "5    6      Smriti Mandhana  IND    714\n",
      "6    7  Chamari Athapaththu   SL    673\n",
      "7    8         Ellyse Perry  AUS    626\n",
      "8    9       Tammy Beaumont  ENG    595\n",
      "9   10      Stafanie Taylor   WI    588\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def top_batsmen(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "    ranking=[]\n",
    "    # Get the first ranking from a different class\n",
    "    first_ranking_element = soup.find('span', class_='rankings-block__pos-number')\n",
    "    first_ranking = first_ranking_element.text.strip()\n",
    "    ranking.append(first_ranking)\n",
    "    \n",
    "    # Get the remaining rankings\n",
    "    ranking_elements = soup.find_all('span', class_=\"rankings-table__pos-number\")\n",
    "    for r in ranking_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ranking.append(r.text.strip())\n",
    "    \n",
    "    player=[]\n",
    "     # Get the first player from a different class\n",
    "    first_player_element = soup.find('div', class_=\"rankings-block__banner--name-large\")\n",
    "    first_player = first_player_element.text.strip()\n",
    "    player.append(first_player)\n",
    "    \n",
    "    # Get the remaining players\n",
    "    player_elements = soup.find_all('td', class_=\"table-body__cell rankings-table__name name\")\n",
    "    for p in player_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        player.append(p.text.strip())\n",
    "    \n",
    "    teams=[]\n",
    "    # Get the first player team from a different class\n",
    "    first_team_element = soup.find('div', class_=\"rankings-block__banner--nationality\")\n",
    "    first_team = first_team_element.text.strip()\n",
    "    teams.append(first_team)\n",
    "    \n",
    "    # Get the remaining teams\n",
    "    team_elements = soup.find_all('span', class_='table-body__logo-text')\n",
    "    for t in team_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        teams.append(t.text.strip())\n",
    "    \n",
    "   \n",
    "    ratings = []\n",
    "    \n",
    "    # Get the first rating from a different class\n",
    "    first_rating_element = soup.find('div', class_='rankings-block__banner--rating')\n",
    "    first_rating = first_rating_element.text.strip()\n",
    "    ratings.append(first_rating)\n",
    "    \n",
    "    # Get the remaining ratings\n",
    "    rank_elements = soup.find_all('td', class_=\"table-body__cell rating\")\n",
    "    for rank in rank_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ratings.append(rank.text.strip())\n",
    "    \n",
    "    df = pd.DataFrame({'Rank': ranking,'Player':player,\"Team\":teams,'Rating':ratings})\n",
    "    return df\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "df = top_batsmen(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6198c402",
   "metadata": {},
   "source": [
    "#### 4) c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "358caa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Rank             Player Team Rating\n",
      "0    1    Hayley Matthews   WI    373\n",
      "1    2     Natalie Sciver  ENG    371\n",
      "2    3       Ellyse Perry  AUS    366\n",
      "3    4     Marizanne Kapp   SA    349\n",
      "4    5        Amelia Kerr   NZ    336\n",
      "5    6      Deepti Sharma  IND    322\n",
      "6    7   Ashleigh Gardner  AUS    292\n",
      "7    8      Jess Jonassen  AUS    250\n",
      "8    9           Nida Dar  PAK    232\n",
      "9   10  Sophie Ecclestone  ENG    205\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def allrounder(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        \n",
    "    ranking=[]\n",
    "    # Get the first ranking from a different class\n",
    "    first_ranking_element = soup.find('span', class_='rankings-block__pos-number')\n",
    "    first_ranking = first_ranking_element.text.strip()\n",
    "    ranking.append(first_ranking)\n",
    "    \n",
    "    # Get the remaining rankings\n",
    "    ranking_elements = soup.find_all('span', class_=\"rankings-table__pos-number\")\n",
    "    for r in ranking_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ranking.append(r.text.strip())\n",
    "    \n",
    "    player=[]\n",
    "     # Get the first player from a different class\n",
    "    first_player_element = soup.find('div', class_=\"rankings-block__banner--name-large\")\n",
    "    first_player = first_player_element.text.strip()\n",
    "    player.append(first_player)\n",
    "    \n",
    "    # Get the remaining players\n",
    "    player_elements = soup.find_all('td', class_=\"table-body__cell rankings-table__name name\")\n",
    "    for p in player_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        player.append(p.text.strip())\n",
    "    \n",
    "    teams=[]\n",
    "    # Get the first player team from a different class\n",
    "    first_team_element = soup.find('div', class_=\"rankings-block__banner--nationality\")\n",
    "    first_team = first_team_element.text.strip()\n",
    "    teams.append(first_team)\n",
    "    \n",
    "    # Get the remaining teams\n",
    "    team_elements = soup.find_all('span', class_='table-body__logo-text')\n",
    "    for t in team_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        teams.append(t.text.strip())\n",
    "    \n",
    "   \n",
    "    ratings = []\n",
    "    \n",
    "    # Get the first rating from a different class\n",
    "    first_rating_element = soup.find('div', class_='rankings-block__banner--rating')\n",
    "    first_rating = first_rating_element.text.strip()\n",
    "    ratings.append(first_rating)\n",
    "    \n",
    "    # Get the remaining ratings\n",
    "    rank_elements = soup.find_all('td', class_=\"table-body__cell rating\")\n",
    "    for rank in rank_elements[0:9]:  # Start from the second rank and go up to the tenth rank\n",
    "        ratings.append(rank.text.strip())\n",
    "    \n",
    "    df = pd.DataFrame({'Rank': ranking,'Player':player,\"Team\":teams,'Rating':ratings})\n",
    "    return df\n",
    "\n",
    "url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "df = allrounder(url)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b05edbd",
   "metadata": {},
   "source": [
    "#### 5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5cab49eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             Headline          Time  \\\n",
      "0   State Farm to stop accepting homeowners insura...   3 Hours Ago   \n",
      "1   How Janie Deegan built Janie's Life-Changing B...   8 Hours Ago   \n",
      "2   Top 10 cheapest places in the U.S. to buy a be...   8 Hours Ago   \n",
      "3   Mark Cuban calls Elon Musk’s Twitter algorithm...   8 Hours Ago   \n",
      "4   3 investing tips as the federal debt ceiling '...   9 Hours Ago   \n",
      "5   Microsoft keyboard users are ‘devastated’ afte...   9 Hours Ago   \n",
      "6   Steve Adcock: The 3 'stupidest' myths I've hea...   9 Hours Ago   \n",
      "7   Chip stocks AMD and Nvidia are among the most ...  10 Hours Ago   \n",
      "8   Analysts are pounding the table for these must...  10 Hours Ago   \n",
      "9   Lower-income consumers pay for wealthy's credi...  10 Hours Ago   \n",
      "10               How Mastercard has outperformed Visa  10 Hours Ago   \n",
      "11  Why commercial real estate firms are joining t...  10 Hours Ago   \n",
      "12  Japan stocks are on fire this year. Why the ra...  10 Hours Ago   \n",
      "13  Marvell Technology shares surge after earnings...  May 26, 2023   \n",
      "14  The tech trade is back, driven by A.I. craze a...  May 26, 2023   \n",
      "15  Next week hints at only short-lived debt deal ...  May 26, 2023   \n",
      "16  Treasury says it could run out of money June 5...  May 26, 2023   \n",
      "17  Disney rips DeSantis bid to disqualify judge i...  May 26, 2023   \n",
      "18  Why the pause on student loan payments has bee...  May 26, 2023   \n",
      "19  Facebook-Giphy sale shows how fear of regulato...  May 26, 2023   \n",
      "20  JPMorgan Chase cut about 500 jobs this week, i...  May 26, 2023   \n",
      "21  Is there a 'right' age for kids to be on socia...  May 26, 2023   \n",
      "22  A.I. excitement leads to a winning week for Nv...  May 26, 2023   \n",
      "23  Nvidia shares jumped 25% this week — and got c...  May 26, 2023   \n",
      "24  Needham says this stock plays the 'almost perf...  May 26, 2023   \n",
      "25  AI is the latest buzzword in tech—but before i...  May 26, 2023   \n",
      "26  Taylor Swift to Metallica: Top 10 most in-dema...  May 26, 2023   \n",
      "27  Investors shifted into these gold and small-ca...  May 26, 2023   \n",
      "28  Ford's EV charging deal with Tesla puts pressu...  May 26, 2023   \n",
      "29  Paramount shares pop after BDT Capital bets on...  May 26, 2023   \n",
      "\n",
      "                                            News Link  \n",
      "0   https://www.cnbc.com/2023/05/27/state-farm-to-...  \n",
      "1   https://www.cnbc.com/2023/05/27/how-janie-deeg...  \n",
      "2   https://www.cnbc.com/2023/05/27/cheapest-place...  \n",
      "3   https://www.cnbc.com/2023/05/27/mark-cuban-say...  \n",
      "4   https://www.cnbc.com/2023/05/27/how-to-invest-...  \n",
      "5   https://www.cnbc.com/2023/05/27/microsoft-keyb...  \n",
      "6   https://www.cnbc.com/2023/05/27/steve-adcock-t...  \n",
      "7   https://www.cnbc.com/2023/05/27/chip-stocks-am...  \n",
      "8   https://www.cnbc.com/2023/05/27/analysts-are-p...  \n",
      "9   https://www.cnbc.com/2023/05/27/lower-income-a...  \n",
      "10  https://www.cnbc.com/2023/05/27/how-mastercard...  \n",
      "11  https://www.cnbc.com/2023/05/27/commercial-rea...  \n",
      "12  https://www.cnbc.com/2023/05/27/japan-stocks-a...  \n",
      "13  https://www.cnbc.com/2023/05/26/marvell-techno...  \n",
      "14  https://www.cnbc.com/2023/05/26/tech-stocks-ar...  \n",
      "15  https://www.cnbc.com/2023/05/26/next-week-hint...  \n",
      "16  https://www.cnbc.com/2023/05/26/treasury-says-...  \n",
      "17  https://www.cnbc.com/2023/05/26/disney-rips-de...  \n",
      "18  https://www.cnbc.com/2023/05/26/the-pause-on-s...  \n",
      "19  https://www.cnbc.com/2023/05/26/facebook-giphy...  \n",
      "20  https://www.cnbc.com/2023/05/26/job-cuts-jpmor...  \n",
      "21  https://www.cnbc.com/2023/05/26/the-right-age-...  \n",
      "22  https://www.cnbc.com/2023/05/26/ai-excitement-...  \n",
      "23  https://www.cnbc.com/2023/05/26/nvidia-shares-...  \n",
      "24  https://www.cnbc.com/2023/05/26/needham-this-s...  \n",
      "25  https://www.cnbc.com/2023/05/26/chatgpt-machin...  \n",
      "26  https://www.cnbc.com/2023/05/26/taylor-swift-b...  \n",
      "27  https://www.cnbc.com/2023/05/26/investors-shif...  \n",
      "28  https://www.cnbc.com/2023/05/26/ford-tesla-ev-...  \n",
      "29  https://www.cnbc.com/2023/05/26/paramount-shar...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def cnbc_news(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content)\n",
    "    \n",
    "    headlines=[]\n",
    "    \n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        headlines.append(i.get('title'))\n",
    "    \n",
    "    time = []\n",
    "    \n",
    "    for i in soup.find_all('time',class_=\"LatestNews-timestamp\"):\n",
    "        time.append(i.text)\n",
    "    link=[]\n",
    "    for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "        link.append(i.get('href'))\n",
    "\n",
    "    df = pd.DataFrame({'Headline':headlines,'Time': time,'News Link':link})\n",
    "    return df\n",
    "    \n",
    "    \n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "df = cnbc_news(url)\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b06ea8",
   "metadata": {},
   "source": [
    "#### 6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "\n",
    "Scrape below mentioned details and make data frame\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "700bdaa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                              Authors  Published date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                         Tim Miller    February 2019   \n",
      "2                                  Margaret A. Boden      August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                     Ilaria Tiddi, Stefan Schlobach     January 2022   \n",
      "5                     Henry Prakken, Giovanni Sartor     October 2015   \n",
      "6    Richard S. Sutton, Doina Precup, Satinder Singh      August 1999   \n",
      "7          Kjersti Aas, Martin Jullum, Anders Løland   September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                      Saurabh Arora, Prashant Doshi      August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12  Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz    November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                        Ron Kohavi, George H. John    December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18   Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz        June 2021   \n",
      "19                            Luigia Carlucci Aiello        June 2016   \n",
      "20            Patrick Lin, Keith Abney, George Bekey       April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                                  URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def top_article(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content)\n",
    "    \n",
    "    title=[]\n",
    "    \n",
    "    for i in soup.find_all('h2',class_=\"sc-1qrq3sd-1 gRGSUS sc-1nmom32-0 sc-1nmom32-1 btcbYu goSKRg\"):\n",
    "        title.append(i.text)\n",
    "    \n",
    "    authors=[]\n",
    "    for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 dnCnAO\"):\n",
    "        authors.append(i.text)\n",
    "    date = []\n",
    "    \n",
    "    for i in soup.find_all('span',class_=\"sc-1thf9ly-2 dvggWt\"):\n",
    "        date.append(i.text)\n",
    "    link=[]\n",
    "    for i in soup.find_all('a',class_=\"sc-5smygv-0 fIXTHm\"):\n",
    "        link.append(i.get('href'))\n",
    "\n",
    "    df = pd.DataFrame({'Title':title,\"Authors\":authors,\"Published date\":date,\"URL\":link})\n",
    "    return df\n",
    "    \n",
    "    \n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "df = top_article(url)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6356ac",
   "metadata": {},
   "source": [
    "#### 7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7e3da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am unable to work with dineout.co.in as I live in the UK. I am using deliveroo.uk instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "3f202a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Restaurant Name              Special Offers\n",
      "0                                 PIZZAEXPRESS    40%‬  off selected items\n",
      "1                        Family Kebab Fish Bar    Spend £30Add a free item\n",
      "2                                 SoBe Burger   Spend £15Get free delivery\n",
      "3                                 Chick'N'Bun   Spend £15Get free delivery\n",
      "4             Little Bao Boy @ The Spark House        Spend £20Get 20% off\n",
      "5                            The Wing Brothers        Spend £20Get 10% off\n",
      "6                           Pizza Hut Delivery           Buy 1, get 1 free\n",
      "7                                    Chaiiwala        Spend £20Get 20% off\n",
      "8                                    Patty Guy        Spend £20Get 20% off\n",
      "9                            Cookies and Cream  Spend £20Get free delivery\n",
      "10                                Grill Hustle  Spend £20Get free delivery\n",
      "11                         Salamis Kebab House  Spend £15Get free delivery\n",
      "12                      Express Food and Wine.    20%‬  off selected items\n",
      "13                               Milano Pizza            Buy 1, get 1 free\n",
      "14                               Chilli Flames        Spend £20Get 25% off\n",
      "15                                 Dessert Inn           Buy 1, get 1 free\n",
      "16                      Pizza Klub Grill House           Buy 1, get 1 free\n",
      "17                   Mr.Brown Pizzaria & Grill           Buy 1, get 1 free\n",
      "18                         L'angolo Dei Sapori        Spend £15Get 20% off\n",
      "19                  Ice Cream Store (Chigwell)        Spend £15Get 20% off\n",
      "20                                  Halal Thai        Spend £15Get 20% off\n",
      "21  Papa Nadox Kitchen (Seafood and Soul food)           Buy 1, get 1 free\n",
      "22                              Chick 'N' Bun   Spend £15Get free delivery\n",
      "23                         NYC Dogs and Shakes        Spend £10Get 10% off\n",
      "24                 24/7 Alcohol Runners London        Spend £30Get 30% off\n",
      "25          The Leytonstone Indian Kitchen LTD        Spend £15Get 20% off\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def deliveroo(url):\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content)\n",
    "    \n",
    "    name=[]\n",
    "    \n",
    "    for i in soup.find_all('p',class_='ccl-649204f2a8e630fd ccl-a396bc55704a9c8a ccl-ff5caa8a6f2b96d0 ccl-40ad99f7b47f3781'):\n",
    "        name.append(i.text)\n",
    "    \n",
    "    offers=[]\n",
    "    \n",
    "    for i in soup.find_all('div','HomeFeedUICard-d38caa5cc97794b4'):\n",
    "        offers.append(i.text)\n",
    "    \n",
    "        \n",
    "    df = pd.DataFrame({'Restaurant Name':name,\"Special Offers\":offers})\n",
    "    return df\n",
    "    \n",
    "    \n",
    "url = 'https://deliveroo.co.uk/restaurants/london/woodford/?geohash=u10j83y9myd1&collection=basket+discounts'\n",
    "df = deliveroo(url)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
